{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deneme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import DatasetDict, Dataset\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data/tr_wiki67/trwiki-67.test.raw',\n",
       " 'data/tr_wiki67/trwiki-67.train.raw',\n",
       " 'data/tr_wiki67/trwiki-67.val.raw']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prefix = \"data/tr_wiki67/trwiki-67.\"\n",
    "file_paths = [ f\"{prefix}{postfix}.raw\" for postfix in [\"test\", \"train\", \"val\"] ]\n",
    "file_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Türkiye', 'İran55', 'ilişkileri', '55_55', '987']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_sentence = \"Türkiye-İran55 ilişkileri.55_55 - 987@\"\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "words = tokenizer.tokenize(example_sentence)\n",
    "words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test for prepare.py script (get_raw_dataset functionality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw dataset is not already exist, it will be created...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1706e5650be48c39ced7a3f1d82e317",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/2 shards):   0%|          | 0/393794 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['examples', 'titles', 'length'],\n",
       "    num_rows: 393794\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import data.tr_wiki67.prepare as prepare\n",
    "\n",
    "prepare.get_raw_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "ds = load_from_disk(\"data/tr_wiki67/raw_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Yolören, Zara ==\n",
      "Yolören, Sivas ilinin Zara ilçesine bağlı bir köydür.\n",
      "Sivas iline 92 km, Zara ilçesine 22 km uzaklıktadır\n",
      "105\n"
     ]
    }
   ],
   "source": [
    "print(ds[\"titles\"][21])\n",
    "print(ds[\"examples\"][21])\n",
    "print(ds[\"length\"][21])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Yolören, Zara ==\n",
      "Yolören, Sivas ilinin Zara ilçesine bağlı bir köydür.\n",
      "Sivas iline 92 km, Zara ilçesine 22 km uzaklıktadır\n",
      "105\n"
     ]
    }
   ],
   "source": [
    "print(ds[\"titles\"][21])\n",
    "print(ds[\"examples\"][21])\n",
    "print(ds[\"length\"][21])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Train and Save Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import (\n",
    "    decoders,\n",
    "    models,\n",
    "    normalizers,\n",
    "    pre_tokenizers,\n",
    "    processors,\n",
    "    trainers,\n",
    "    Tokenizer,\n",
    ")\n",
    "\n",
    "tokenizer = Tokenizer(models.WordPiece(unk_token=\"[UNK]\"))\n",
    "\n",
    "tokenizer.normalizer = normalizers.Sequence(\n",
    "    [normalizers.NFKC(),\n",
    "     normalizers.Lowercase()]\n",
    ")\n",
    "\n",
    "# tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel()\n",
    "\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.Sequence([pre_tokenizers.WhitespaceSplit(), \n",
    "                                                   pre_tokenizers.Digits(individual_digits=True),\n",
    "                                                   pre_tokenizers.Punctuation()])\n",
    "\n",
    "\n",
    "special_tokens = [\"[UNK]\", \"[PAD]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n",
    "\n",
    "trainer = trainers.WordPieceTrainer(vocab_size=25_000, special_tokens=special_tokens, min_frequency=5,\n",
    "                                    continuing_subword_prefix=\"##\", limit_alphabet=500)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "?   merhabalar    555555 benim8 adım! ! .muhammet can gümüşsuüğği̇i̇çç 55  ة الزلزلة ?? ?\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.normalizer.normalize_str(\"?   Merhabalar    555555 benim8 adım! ! .Muhammet Can GümüşsuÜğĞİİÇç 55  ة الزلزلة ?? ?\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('?', (0, 1)),\n",
       " ('Merhabalar', (4, 14)),\n",
       " ('5', (18, 19)),\n",
       " ('5', (19, 20)),\n",
       " ('5', (20, 21)),\n",
       " ('5', (21, 22)),\n",
       " ('5', (22, 23)),\n",
       " ('5', (23, 24)),\n",
       " ('benim', (25, 30)),\n",
       " ('8', (30, 31)),\n",
       " ('adım', (32, 36)),\n",
       " ('!', (36, 37)),\n",
       " ('!', (38, 39)),\n",
       " ('.', (40, 41)),\n",
       " ('Muhammet', (41, 49)),\n",
       " ('Can', (50, 53)),\n",
       " ('GümüşsuÜğĞİİÇç', (54, 68)),\n",
       " ('5', (69, 70)),\n",
       " ('5', (70, 71)),\n",
       " ('ة', (73, 74)),\n",
       " ('الزلزلة', (75, 82)),\n",
       " ('?', (83, 84)),\n",
       " ('?', (84, 85)),\n",
       " ('?', (86, 87))]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pre_tokenizer.pre_tokenize_str(\"?   Merhabalar    555555 benim8 adım! ! .Muhammet Can GümüşsuÜğĞİİÇç 55  ة الزلزلة ?? ?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.tr_wiki67.prepare import get_raw_dataset\n",
    "\n",
    "dataset = get_raw_dataset()\n",
    "\n",
    "def get_example_batch():\n",
    "    \"\"\"\n",
    "    generator func\n",
    "    will be used when training tokenizer\n",
    "    \"\"\"\n",
    "    for i in range(0, len(dataset), 1000):\n",
    "        yield dataset[i : i + 1000][\"examples\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.train_from_iterator(get_example_batch(), trainer=trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['?', 'mer', '##ha', '##bal', '##ar', '5', '5', '5', '5', '5', '5', 'benim', '8', 'adım', '!', '!', '.', 'muham', '##met', 'can', 'gümüş', '##su', '##üğ', '##ği', '##̇', '##i̇', '##ç', '##ç', '5', '5', 'ة', 'ال', '##ز', '##ل', '##ز', '##ل', '##ة', '?', '?', '?']\n"
     ]
    }
   ],
   "source": [
    "encoding = tokenizer.encode(\"?   Merhabalar    555555 benim8 adım! ! .Muhammet Can GümüşsuÜğĞİİÇç 55  ة الزلزلة ?? ?\")\n",
    "print(encoding.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding(num_tokens=40, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])\n",
      "[33, 2190, 1087, 7264, 941, 25, 25, 25, 25, 25, 25, 4269, 28, 5038, 5, 5, 18, 3981, 1416, 2051, 4222, 3240, 13252, 1046, 549, 5982, 560, 560, 25, 25, 316, 8854, 605, 592, 605, 592, 596, 33, 33, 33]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "['?', 'mer', '##ha', '##bal', '##ar', '5', '5', '5', '5', '5', '5', 'benim', '8', 'adım', '!', '!', '.', 'muham', '##met', 'can', 'gümüş', '##su', '##üğ', '##ği', '##̇', '##i̇', '##ç', '##ç', '5', '5', 'ة', 'ال', '##ز', '##ل', '##ز', '##ل', '##ة', '?', '?', '?']\n",
      "[(0, 1), (4, 7), (7, 9), (9, 12), (12, 14), (18, 19), (19, 20), (20, 21), (21, 22), (22, 23), (23, 24), (25, 30), (30, 31), (32, 36), (36, 37), (38, 39), (40, 41), (41, 46), (46, 49), (50, 53), (54, 59), (59, 61), (61, 63), (63, 65), (64, 65), (65, 66), (66, 67), (67, 68), (69, 70), (70, 71), (73, 74), (75, 77), (77, 78), (78, 79), (79, 80), (80, 81), (81, 82), (83, 84), (84, 85), (86, 87)]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(encoding)\n",
    "print(encoding.ids)\n",
    "print(encoding.type_ids)\n",
    "print(encoding.tokens)\n",
    "print(encoding.offsets)\n",
    "print(encoding.attention_mask)\n",
    "print(encoding.special_tokens_mask)\n",
    "print(encoding.overflowing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 3\n"
     ]
    }
   ],
   "source": [
    "cls_token_id = tokenizer.token_to_id(\"[CLS]\")\n",
    "sep_token_id = tokenizer.token_to_id(\"[SEP]\") \n",
    "\n",
    "print(cls_token_id, sep_token_id)\n",
    "\n",
    "\n",
    "tokenizer.post_processor = processors.TemplateProcessing(\n",
    "    single=f\"[CLS]:0 $A:0 [SEP]:0\",\n",
    "    pair=f\"[CLS]:0 $A:0 [SEP]:0 $B:1 [SEP]:1\",\n",
    "    special_tokens=[(\"[CLS]\", cls_token_id), (\"[SEP]\", sep_token_id)]\n",
    ")\n",
    "\n",
    "tokenizer.decoder = decoders.WordPiece(prefix=\"##\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding(num_tokens=23, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])\n",
      "[2, 2190, 1087, 1646, 4269, 5038, 24069, 18, 18, 18, 3, 1223, 12969, 8048, 529, 15588, 4269, 5038, 1129, 18, 18, 18, 3]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "['[CLS]', 'mer', '##ha', '##ba', 'benim', 'adım', 'name', '.', '.', '.', '[SEP]', 'as', 'gard', '##aşı', '##m', 'peki', 'benim', 'adım', 'ne', '.', '.', '.', '[SEP]']\n",
      "[(0, 0), (1, 4), (4, 6), (6, 8), (9, 14), (15, 19), (20, 24), (24, 25), (25, 26), (26, 27), (0, 0), (1, 3), (4, 8), (8, 11), (11, 12), (13, 17), (18, 23), (24, 28), (29, 31), (31, 32), (32, 33), (33, 34), (0, 0)]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "encoding = tokenizer.encode(\" merhaba benim adım name...\" , \" AS gardaşım peki benim adım ne...\")\n",
    "print(encoding)\n",
    "print(encoding.ids)\n",
    "print(encoding.type_ids)\n",
    "print(encoding.tokens)\n",
    "print(encoding.offsets)\n",
    "print(encoding.attention_mask)\n",
    "print(encoding.special_tokens_mask)\n",
    "print(encoding.overflowing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'merhaba benim adım name... as gardaşım peki benim adım ne...'"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(encoding.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.449533407710257\n"
     ]
    }
   ],
   "source": [
    "with open(\"data/tr_wiki67/trwiki-67.test.raw\", \"+r\") as f:\n",
    "    content = f.read()\n",
    "    total_num_char = len(content)\n",
    "    acc = len(tokenizer.encode(content).tokens)\n",
    "    print(total_num_char / acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    }
   ],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "wrapped_tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_object=tokenizer,\n",
    "    # tokenizer_file=\"tokenizer/tokenizer_55.json\", # You can load from the tokenizer file, alternatively\n",
    "    unk_token=\"[UNK]\",\n",
    "    pad_token=\"[PAD]\",\n",
    "    cls_token=\"[CLS]\",\n",
    "    sep_token=\"[SEP]\",\n",
    "    mask_token=\"[MASK]\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrapped_tokenizer.is_fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('fast_wordpiece\\\\tokenizer_config.json',\n",
       " 'fast_wordpiece\\\\special_tokens_map.json',\n",
       " 'fast_wordpiece\\\\tokenizer.json')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrapped_tokenizer.save_pretrained(\"fast_wordpiece\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "321\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " '4',\n",
       " 'haziran',\n",
       " '1',\n",
       " '9',\n",
       " '8',\n",
       " '0',\n",
       " \"'\",\n",
       " 'de',\n",
       " 'i̇zmir',\n",
       " \"'\",\n",
       " 'de',\n",
       " 'doğdu',\n",
       " '.',\n",
       " 'meli',\n",
       " '##h',\n",
       " 'öz',\n",
       " '##akat',\n",
       " 'i̇lkokulu',\n",
       " \"'\",\n",
       " 'na',\n",
       " ',',\n",
       " 'özel',\n",
       " 'çak',\n",
       " '##abe',\n",
       " '##y',\n",
       " 'lisesi',\n",
       " \"'\",\n",
       " 'nde',\n",
       " 'ortaokul',\n",
       " '##a',\n",
       " 've',\n",
       " 'konservatuvar',\n",
       " '##da',\n",
       " 'lise',\n",
       " '##ye',\n",
       " 'gitti',\n",
       " '.',\n",
       " '7',\n",
       " 'yaşında',\n",
       " 'okul',\n",
       " 'hayatına',\n",
       " 'başlamasıyla',\n",
       " 'trt',\n",
       " 've',\n",
       " 'i̇zmir',\n",
       " 'devlet',\n",
       " 'konservatuvarı',\n",
       " \"'\",\n",
       " 'nın',\n",
       " 'çocuk',\n",
       " 'koro',\n",
       " '##larında',\n",
       " 'müzik',\n",
       " 'eğitimine',\n",
       " 'başlaması',\n",
       " 'aynı',\n",
       " 'zamana',\n",
       " 'denk',\n",
       " 'gelir',\n",
       " '.',\n",
       " 'i̇lk',\n",
       " 'geri',\n",
       " 'vokal',\n",
       " 'deneyimini',\n",
       " '1',\n",
       " '2',\n",
       " 'yaşında',\n",
       " 'sezen',\n",
       " 'aksu',\n",
       " \"'\",\n",
       " 'yla',\n",
       " 'beraber',\n",
       " '1',\n",
       " '9',\n",
       " '9',\n",
       " '2',\n",
       " 'yılında',\n",
       " 'gerçekleştirdi',\n",
       " '.',\n",
       " 'sezen',\n",
       " 'aksu',\n",
       " \"'\",\n",
       " 'nun',\n",
       " 'yanında',\n",
       " 'solo',\n",
       " 've',\n",
       " 'vokal',\n",
       " 'çalışmaları',\n",
       " 'devam',\n",
       " 'ederken',\n",
       " 'okul',\n",
       " 'hayatı',\n",
       " 'i̇zmir',\n",
       " \"'\",\n",
       " 'de',\n",
       " 'devam',\n",
       " 'etmekteydi',\n",
       " '.',\n",
       " 'i̇lk',\n",
       " 'tv',\n",
       " 'programına',\n",
       " 'yine',\n",
       " 'sezen',\n",
       " 'aksu',\n",
       " \"'\",\n",
       " 'yla',\n",
       " '1',\n",
       " '9',\n",
       " '9',\n",
       " '2',\n",
       " 'yılında',\n",
       " 'trt',\n",
       " '’',\n",
       " 'de',\n",
       " 'çıktı',\n",
       " '.',\n",
       " 'geri',\n",
       " 'vokal',\n",
       " 'çalışmalarının',\n",
       " 'yanı',\n",
       " 'sıra',\n",
       " 'bu',\n",
       " 'programda',\n",
       " 'solo',\n",
       " 'bir',\n",
       " 'şarkı',\n",
       " 'da',\n",
       " 'seslendirme',\n",
       " '##si',\n",
       " 'kamuoyunda',\n",
       " 'büyük',\n",
       " 'ilgi',\n",
       " 'gördü',\n",
       " '.',\n",
       " 'büyük',\n",
       " 'yetenek',\n",
       " 'olarak',\n",
       " 'lans',\n",
       " '##e',\n",
       " 'edilen',\n",
       " 'tuğ',\n",
       " '##ba',\n",
       " 'özerk',\n",
       " \"'\",\n",
       " 'in',\n",
       " 'sahne',\n",
       " 've',\n",
       " 'müzik',\n",
       " 'hayatı',\n",
       " 'tam',\n",
       " 'anlamıyla',\n",
       " 'böyle',\n",
       " 'başladı',\n",
       " 'den',\n",
       " '##ilebilir',\n",
       " '.',\n",
       " 'okul',\n",
       " 'hayatı',\n",
       " 'bittikten',\n",
       " 'sonra',\n",
       " 'i̇stanbul',\n",
       " '’',\n",
       " 'a',\n",
       " 'taşınan',\n",
       " 'sanatçı',\n",
       " 'aralarında',\n",
       " 'ege',\n",
       " ',',\n",
       " 'deniz',\n",
       " 'sek',\n",
       " '##i',\n",
       " 'gibi',\n",
       " 'isimler',\n",
       " 'bulunan',\n",
       " 'sanatçılar',\n",
       " '##a',\n",
       " 'geri',\n",
       " 'vokal',\n",
       " 'yaptı',\n",
       " '.',\n",
       " 'daha',\n",
       " 'sonra',\n",
       " 'solo',\n",
       " 'sahne',\n",
       " 'çalışmalarına',\n",
       " 'başlayan',\n",
       " 'özerk',\n",
       " ',',\n",
       " '2',\n",
       " '0',\n",
       " '0',\n",
       " '2',\n",
       " 'yılının',\n",
       " 'aralık',\n",
       " 'ayında',\n",
       " '\"',\n",
       " 'dün',\n",
       " 'gibi',\n",
       " '\"',\n",
       " 'adlı',\n",
       " 'ilk',\n",
       " 'solo',\n",
       " 'albümünü',\n",
       " 'çıkardı',\n",
       " '.',\n",
       " \"'\",\n",
       " 'aşk',\n",
       " 'yara',\n",
       " '##sı',\n",
       " \"'\",\n",
       " 'adlı',\n",
       " 'şarkısıyla',\n",
       " 'uzun',\n",
       " 'süre',\n",
       " 'radyo',\n",
       " '##larda',\n",
       " 'liste',\n",
       " 'başlarında',\n",
       " 'yer',\n",
       " 'aldı',\n",
       " '.',\n",
       " 'albümle',\n",
       " 'beraber',\n",
       " 'oyunculuk',\n",
       " 'yönünü',\n",
       " 'göster',\n",
       " '##ebilme',\n",
       " 'fırsatı',\n",
       " 'yakalayan',\n",
       " 'sanatçı',\n",
       " ',',\n",
       " 'atv',\n",
       " \"'\",\n",
       " 'nin',\n",
       " 'uzun',\n",
       " 'süredir',\n",
       " 'yayın',\n",
       " 'yapmış',\n",
       " 'olduğu',\n",
       " 'dizisi',\n",
       " '\"',\n",
       " 'böyle',\n",
       " 'mi',\n",
       " 'olacaktı',\n",
       " '\"',\n",
       " 'da',\n",
       " 'bir',\n",
       " 'sene',\n",
       " 'kadar',\n",
       " 'rol',\n",
       " 'aldı',\n",
       " '.',\n",
       " 'i̇lk']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = \"\"\"4 Haziran 1980'de İzmir'de doğdu. Melih Özakat İlkokulu'na, Özel Çakabey Lisesi'nde ortaokula ve konservatuvarda liseye gitti. 7 yaşında okul hayatına başlamasıyla TRT ve İzmir Devlet Konservatuvarı'nın çocuk korolarında müzik eğitimine başlaması aynı zamana denk gelir. İlk geri vokal deneyimini 12 yaşında Sezen Aksu'yla beraber 1992 yılında gerçekleştirdi. Sezen Aksu'nun yanında solo ve vokal çalışmaları devam ederken okul hayatı İzmir'de devam etmekteydi.\n",
    "İlk TV programına yine Sezen Aksu'yla 1992 yılında TRT’de çıktı. Geri vokal çalışmalarının yanı sıra bu programda solo bir şarkı da seslendirmesi kamuoyunda büyük ilgi gördü. Büyük yetenek olarak lanse edilen Tuğba Özerk'in sahne ve müzik hayatı tam anlamıyla böyle başladı denilebilir. Okul hayatı bittikten sonra İstanbul’a taşınan sanatçı aralarında Ege, Deniz Seki gibi isimler bulunan sanatçılara geri vokal yaptı. Daha sonra solo sahne çalışmalarına başlayan Özerk, 2002 yılının Aralık ayında \"Dün Gibi\" adlı ilk solo albümünü çıkardı. 'Aşk yarası' adlı şarkısıyla uzun süre radyolarda liste başlarında yer aldı. Albümle beraber oyunculuk yönünü gösterebilme fırsatı yakalayan sanatçı, atv'nin uzun süredir yayın yapmış olduğu dizisi \"Böyle mi olacaktı\"da bir sene kadar rol aldı. İlk albümünde bir tane bestesi olan Özerk üç senelik boşlukta birçok besteye de imza attı.6 Temmuz 2005 tarihinde 'Lo Lo Lo' adlı ikinci solo albümünü çıkardı.'Lo Lo Lo' adlı şarkısıyla uzun süre radyolarda liste başı oldu. İkinci albümünde bir tane bestesi olan iki senelik boşlukta birçok besteye de imza attı\n",
    "\"\"\"\n",
    "encoding = wrapped_tokenizer(test)\n",
    "print(len(encoding.tokens()))   # our tokenizer does not have any limit-boundry about block_size\n",
    "encoding.tokens()[:254]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data For (NSP-MLM) Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, load_from_disk, load_dataset\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nsp_mlm_dataset = { \"a_b_text\": [], \"x\": [], \"y\": [], \"att_mask\": [], \"token_type\": [], \"isNext\": [] }\n",
    "\n",
    "isNext = None\n",
    "x = []\n",
    "y = []\n",
    "attention_mask = []\n",
    "token_type = []\n",
    "token_id = []\n",
    "a_b_text = []\n",
    "\n",
    "block_size = 255\n",
    "A_segment_flag = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_table = load_from_disk(\"data/tr_wiki67/raw_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pairs(raw_row):\n",
    "    # split example by sentences\n",
    "    list_of_sentences = sent_tokenize(raw_row[\"examples\"])\n",
    "    A = []\n",
    "    B = []\n",
    "\n",
    "    for pair_index in range(0, len(list_of_sentences)-1):\n",
    "        A.append(list_of_sentences[pair_index])\n",
    "        B.append(list_of_sentences[pair_index + 1])\n",
    "\n",
    "    return {\"A\": A, \"B\": B}\n",
    "\n",
    "\n",
    "def create_a_b_table(raw_table_path):\n",
    "    \"\"\"\n",
    "    TODO: ara kayıt işlemleri yap (save as arrow), hali hazırda var ise yükle ve döndür, examples, titles, length kolonlarını sil\n",
    "    \"\"\"\n",
    "    # load raw dataset from disk\n",
    "    raw_table = load_from_disk(raw_table_path)\n",
    "    # # bu satır geçici! test'i hızlı yapabilmek için\n",
    "    # raw_table = Dataset.from_dict(raw_table[:5_000])\n",
    "    # every example needs at least 2 sentence\n",
    "    raw_table_filtered = raw_table.filter(lambda row:  len(sent_tokenize(row[\"examples\"], language=\"turkish\")) > 1)\n",
    "\n",
    "\n",
    "    # create table that contains A, B kols (problem: row of this table has list of sentences not just one sentence for each pair)\n",
    "    a_b_list_table = raw_table_filtered.map(create_pairs)\n",
    "\n",
    "\n",
    "    # \n",
    "    A, B = [], []\n",
    "    for A_list, B_list in zip(a_b_list_table[\"A\"], a_b_list_table[\"B\"]):\n",
    "        A += A_list\n",
    "        B += B_list\n",
    "    \n",
    "    a_b_table = {\"A\":A, \"B\":B}\n",
    "    a_b_table = Dataset.from_dict(a_b_table)\n",
    "\n",
    "    return a_b_table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "table is not already exists, will be created and saved to disk...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "129587c4edca43fa97614c8954c4c76f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/3 shards):   0%|          | 0/4119470 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "raw_table_path = \"data/tr_wiki67/raw_dataset\"\n",
    "ab_table_path = \"deneme_ab_tablosu\"\n",
    "\n",
    "\n",
    "# if table is not already exists\n",
    "if os.path.exists(ab_table_path) == False:\n",
    "    print(\"table is not already exists, will be created and saved to disk...\")\n",
    "    \n",
    "    # create table\n",
    "    a_b_table = create_a_b_table(raw_table_path)\n",
    "\n",
    "    # save created table\n",
    "    a_b_table.save_to_disk(ab_table_path)\n",
    "\n",
    "# ab table is already exists\n",
    "else:\n",
    "    print(\"table is already exists, will be loaded from disk...\")\n",
    "    # load table from disk\n",
    "    a_b_table = load_from_disk(ab_table_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['A', 'B'],\n",
       "    num_rows: 4119470\n",
       "})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_b_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = a_b_table[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: Thomas Lüthi (d. 6 Eylül 1986; Oberdiessbach), İnterwetten Paddock Moto2 takımı için, Moto2 Grand Prix Dünya Şampiyonası'nda yarışan İsviçreli profesyonel bir motosiklet yol yarışçısı., \n",
      "B: 2010 yılından bu yanan Moto2 Dünya Şampiyonası'nda yarışmaktadır.\n",
      "\n",
      "\n",
      "A: 2010 yılından bu yanan Moto2 Dünya Şampiyonası'nda yarışmaktadır., \n",
      "B: Daha önce 250cc Dünya Şampiyonası ve 125cc Dünya Şampiyonası'nda yarışmıştır.\n",
      "\n",
      "\n",
      "A: Daha önce 250cc Dünya Şampiyonası ve 125cc Dünya Şampiyonası'nda yarışmıştır., \n",
      "B: 2005 yılında 125cc Dünya Şampiyonası'nda şampiyon olmuştur.\n",
      "\n",
      "\n",
      "A: 2005 yılında 125cc Dünya Şampiyonası'nda şampiyon olmuştur., \n",
      "B: Tüm istatistikler MotoGP.com sitesinden alınmıştır.\n",
      "\n",
      "\n",
      "A: Laguna Verde, Bolivya'da, Altiplano Platosu'nun güneybatısında bir göldür., \n",
      "B: Licancabur'un eteğinde bulunur.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(f\"A: {temp['A'][i]}, \\nB: {temp['B'][i]}\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NSP-MLM PREP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "import os\n",
    "import random\n",
    "\n",
    "\n",
    "from tokenizers import (\n",
    "    pre_tokenizers,\n",
    "    processors,\n",
    "    Tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "table is already exists, will be loaded from disk...\n"
     ]
    }
   ],
   "source": [
    "# GLOBAL VARS  \n",
    "\n",
    "raw_table_path = \"data/tr_wiki67/raw_dataset\"\n",
    "ab_table_path = \"deneme_ab_tablosu\"\n",
    "block_size = 255\n",
    "\n",
    "tokenizer = Tokenizer.from_file(\"tokenizer_wordpiece/tokenizer.json\")\n",
    "\n",
    "cls_id = tokenizer.token_to_id(\"[CLS]\")\n",
    "sep_id = tokenizer.token_to_id(\"[SEP]\")\n",
    "pad_id = tokenizer.token_to_id(\"[PAD]\")\n",
    "mask_id = tokenizer.token_to_id(\"[MASK]\")\n",
    "\n",
    "# if table is not already exists\n",
    "if os.path.exists(ab_table_path) == False:\n",
    "    print(\"table is not already exists, will be created and saved to disk...\")\n",
    "    \n",
    "    # create table\n",
    "    a_b_table = create_a_b_table(raw_table_path)\n",
    "\n",
    "    # save created table\n",
    "    a_b_table.save_to_disk(ab_table_path)\n",
    "\n",
    "# ab table is already exists\n",
    "else:\n",
    "    print(\"table is already exists, will be loaded from disk...\")\n",
    "    # load table from disk\n",
    "    a_b_table = load_from_disk(ab_table_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _filter_cand_words(row):\n",
    "    return row[\"cand\"] != None\n",
    "\n",
    "def _map_cand_words(row, num_tokens):\n",
    "    ab = row[\"A\"] + row[\"B\"]\n",
    "    ab_words = tokenizer.decode(tokenizer.encode(ab, add_special_tokens=False).ids).split(\" \")\n",
    "\n",
    "    cand = None\n",
    "\n",
    "    for word in ab_words:\n",
    "        word_ids = tokenizer.encode(word).ids\n",
    "        if len(word_ids) == num_tokens:\n",
    "            cand = word_ids\n",
    "            break \n",
    "    \n",
    "    return {\"cand\": cand}\n",
    "\n",
    "\n",
    "def get_random_word_tokens(num_tokens, chunk_size=1_000):\n",
    "    # why chunk? accessing all examples (~4M) for random word is inefficient\n",
    "    # accessing dataset one by one is also inefficient (memory map, disk)\n",
    "    # my solution is, define some chunk, and then apply .map() method\n",
    "    chunk_idx = random.randint(len(a_b_table) - chunk_size)\n",
    "    chunk_ab = a_b_table[chunk_idx: chunk_idx + chunk_size]\n",
    "\n",
    "    candidate_words_table = chunk_ab.map(lambda row: _map_cand_words(row, num_tokens), batched=True).filter(_filter_cand_words, batched=True)\n",
    "    idx = random.random(len(candidate_words_table))\n",
    "    return tokenizer.encode(candidate_words_table[idx])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_nsp_mlm_table(ab_row):\n",
    "    isNext = random.random() > 0.5\n",
    "    x = []  # input sequence\n",
    "    y = []  # target sequence\n",
    "    \n",
    "    if isNext == False:\n",
    "        rand_b = a_b_table[\"B\"][random.randint(0, len(a_b_table))]\n",
    "        ab_row[\"B\"] = rand_b\n",
    "\n",
    "    encoding = tokenizer.encode(ab_row[\"A\"], ab_row[\"B\"])\n",
    "    ab_len = len(encoding.tokens) \n",
    "\n",
    "    if ab_len > block_size:\n",
    "        return None\n",
    "    \n",
    "    for word in tokenizer.decode(encoding.ids, skip_special_tokens=False).split(\" \"):\n",
    "\n",
    "        if word == \"[CLS]\":\n",
    "            x.append(cls_id)\n",
    "            y.append(isNext)\n",
    "\n",
    "        elif word == \"[SEP]\":\n",
    "            x.append(sep_id)\n",
    "            y.append(pad_id)\n",
    "        \n",
    "        else:\n",
    "            tokens = tokenizer.encode(word, add_special_tokens=False).tokens\n",
    "\n",
    "            prob = random.random()\n",
    "            if prob > 0.85:\n",
    "                prob_inner = random.random()\n",
    "\n",
    "                \n",
    "                if prob_inner < 0.8:\n",
    "                    # mask condition\n",
    "                    x += [mask_id for token in tokens]\n",
    "                    y += tokens\n",
    "                \n",
    "                elif prob_inner < 0.9:\n",
    "                    # corrupt condition\n",
    "\n",
    "                    trial_limit = 10\n",
    "                    trial = 1\n",
    "\n",
    "                    while(trial <= trial_limit):\n",
    "                        random_word_tokens = get_random_word_tokens(num_tokens=len(tokens), trial_limit=5000)\n",
    "                        if random_word_tokens != None:\n",
    "                            break\n",
    "                    if random_word_tokens == None:\n",
    "                        print(f\"cannot find word that has {len(tokens)} number of tokens, in this {trial_limit} trials...\")\n",
    "                        return None\n",
    "                    \n",
    "                    x += get_random_word_tokens(num_tokens=len(tokens))\n",
    "                    y += tokens\n",
    "                \n",
    "                else:\n",
    "                    # identity condition\n",
    "                    x += tokens\n",
    "                    y += tokens\n",
    "                \n",
    "            else:\n",
    "                x += [tokenizer.token_to_id(token) for token in tokens]\n",
    "                y += [pad_id for token in tokens]\n",
    "\n",
    "        assert len(x) == len(y), \"[ERROR] there is some mistake, x and y should be the same length!\"\n",
    "\n",
    "        num_pad = block_size - len(x)\n",
    "        x += [pad_id for each in range(num_pad)]\n",
    "        y += [pad_id for each in range(num_pad)]\n",
    "\n",
    "        first_sep_idx = x.index(\"[SEP]\")\n",
    "        token_type_id = [ 0 if i <= first_sep_idx else 1 for i in range(len(x)) ]\n",
    "\n",
    "        attention_mask = [ 0 if token_id == pad_id else 1 for token_id in x ]\n",
    "\n",
    "        return {\"X\": x, \"Y\": y, \"Attention_mask\": attention_mask, \"Token_type_id\":token_type_id, \"isNext\": isNext}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.token_to_id(\"[UNK]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_b_table.map(create_nsp_mlm_table, batched=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['devrim', '##leri']"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenizer.encode(\"devrimleri\", add_special_tokens=False).tokens\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4181, 1034]"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[tokenizer.token_to_id(each) for each in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'A': \"Beş günlük Milan (İtalyanca: Cinque giornate di Milano), İtalya'da 1848 devrimleri sırasında İtalya'nın kuzeyinde bulunan Milano'da gerçekleşen olaylar için kullanılır.\",\n",
       " 'B': \"18 Mart 1848 yılında İtalyalı isyancılar Milano'nun sokaklarına barikat kurup yarımadadaki direnişi buraya taşıdı.\"}"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = a_b_table[989]\n",
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'beş', 'günlük', 'milan', '(', 'i̇talyanca', ':', 'cin', '##que', 'gior', '##nat', '##e', 'di', 'milano', ')', ',', 'i̇talya', \"'\", 'da', '1', '8', '4', '8', 'devrim', '##leri', 'sırasında', 'i̇talya', \"'\", 'nın', 'kuzeyinde', 'bulunan', 'milano', \"'\", 'da', 'gerçekleşen', 'olaylar', 'için', 'kullanılır', '.', '[SEP]', '1', '8', 'mart', '1', '8', '4', '8', 'yılında', 'i̇talya', '##lı', 'isyancı', '##lar', 'milano', \"'\", 'nun', 'sokak', '##larına', 'bar', '##ikat', 'kurup', 'yarımada', '##daki', 'direnişi', 'buraya', 'taşıdı', '.', '[SEP]']\n",
      "['[CLS]', 'beş', 'günlük', 'milan', '(', 'i̇talyanca', ':', 'cinque', 'giornate', 'di', 'milano', '),', 'i̇talya', \"'\", 'da', '1', '8', '4', '8', 'devrimleri', 'sırasında', 'i̇talya', \"'\", 'nın', 'kuzeyinde', 'bulunan', 'milano', \"'\", 'da', 'gerçekleşen', 'olaylar', 'için', 'kullanılır.', '[SEP]', '1', '8', 'mart', '1', '8', '4', '8', 'yılında', 'i̇talyalı', 'isyancılar', 'milano', \"'\", 'nun', 'sokaklarına', 'barikat', 'kurup', 'yarımadadaki', 'direnişi', 'buraya', 'taşıdı.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "encoding = tokenizer.encode(example[\"A\"], example[\"B\"])\n",
    "print(encoding.tokens)\n",
    "words = tokenizer.decode(encoding.ids, skip_special_tokens=False).split(\" \")\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# + A, B tablosu oluştur (textual formatında)\n",
    "# + dataset_nsp_mlm dataset objesi/tablosu oluştur\n",
    "# + dataset_nsp_mlm kolonları-key'leri -> a_b_raw, x, y, att_mask, token_type, isNext\n",
    "# + isNext bool değişkeni, x, y, attention_mask, token_type_id, id, a_b_raw (birleşik row) list objeleri oluştur, block_size değişkeni, A_segment_flag:bool değişkeni olsun\n",
    "# + A, B tablosunda her row'a git\n",
    "# + isNext olasılığı al\n",
    "# + isNext True ise o rowdaki A ve B'yi al, bu iki text'i birleştir (başa [CLS], araya ve sona [SEP] flag kelimeleri gelsin) -> a_b_raw\n",
    "# + isNext False ise o rowdaki A'yı al, rastgele B al (başa [CLS], araya ve sona [SEP] f lag kelimeleri gelsin) -> a_b_raw\n",
    "# + birleşik row'u tokenize et, length'i > block_size ise bu birleşik row'la işimiz yok dataset_nsp_mlm'ye eklenmeyecek, bu example'dan çık (return None)\n",
    "# + A_segment_flag = True değişkeni oluştur\n",
    "# + row alma işleminden sonra, loop döngüsü gir (kelime sayısı kadar döngü)\n",
    "# + eğer kelime [CLS] kelimesi ise x'e [CLS] tokeni koy y'ye isNext değişken değerini(1/0) {kelime vs token ayrımına dikkat}\n",
    "# + eğer kelime [SEP] kelimesi ise x'e [SEP] tokeni koy y'ye [PAD] token_id değerini koy\n",
    "# + kelime [CLS], [SEP] değil ise:\n",
    "# + kelimeyi tokenize et\n",
    "# + olasılık al, olasılık tutmaz ise (mask, corrupt, identity) kelime tokenlarını direkt x'e koy, y'ye her token için [PAD] tokeni koy\n",
    "# + olasılık mask condition'u tuttu ise kelime tokenlarını al y'ye koy, x'e de [MASK] tokenlarını koy (her kelime token'ı için tabi)\n",
    "# + olasılık corrupt condition'u tuttu ise kelime token sayısını al \"get_random_word_tokens(num_tokens=değer)\" çağır. Gelen token'ları x'e koy, kendi kelimemizin token'larını da y'ye koy\n",
    "# + get_random_word_tokens false return etti ise, deneme sayı limiti aşılmış demektir (belirtilen token sayısına uygun kelime bulunamayabilir sonuçta) bu durumda gene bu row/example'dan return null ile çıkalım\n",
    "# + olasılık identity condition'u tuttu ise kelime tokenlarını al x'e ve y'ye koy\n",
    "# + token_type_id listesine ilk [SEP] token ile karşılaşana kadar her x, y doldurumunda her token için 1, bundan sonra hep 0 koyulacak (A_segment_flag kullanılacak)\n",
    "# + döngüden çıktıktan sonra (tüm kelimeleri gezilmiş demek):\n",
    "# + x ve y aynı uzunlukta mı kontrol et, block_size - x uzunluğu kadar [PAD] tokenlı list oluştur, x ve y'ye eklemlendir\n",
    "# + x uzunluğu kadar 1, block_size - x uzunluğu kadar 0 değerli list oluştur -> attention_mask liste bu list'eyi refere edecek\n",
    "# + elimizdeki değişkenler ile (x, y, a_b_raw, attention mask, token type id, isNext) dataset_nsp_mlm dataset'ine yeni bir row döndüreceğiz\n",
    "# + A, B tablosunda tüm rowlara girdikten sonra (.map ile), dataset_nsp_mlm tablosu tamamlanmış demektir. bu tabloyu disk'e kaydet\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ---- A, B Tablosu Oluşturma ----\n",
    "\n",
    "# yeni bir dataset objesi oluştur\n",
    "# klasik dataset objesini alttaki functionalite ile .map'le (her bir row'a git bu fonk uygula, fonk ne return ediyorsa)\n",
    "# pair kerneli oluştur\n",
    "# example metini (doc) sentence'lara split et\n",
    "# döngü başlat (0, kaç tane sentence var ise - 1)\n",
    "# pair kerneli doldur (2 sentence ile)\n",
    "# yeni dataset'e extend'le, ekle \n",
    "\n",
    "\n",
    "# ---- get_random_word_tokens(num_tokens=değer) oluşturma ----#\n",
    "\n",
    "# deneme sayısı değişkeni = 5000 (max kelime bakma sayısı)\n",
    "# deneme döngüsüne gir (0'dan deneme sayısı değişkeni)\n",
    "# + random_row sayı/index al [0, A,B tablosu row sayısı] : range\n",
    "# + random_word sayı/index al [0, o row'da kaç tane kelime var ise] : range\n",
    "# + bu index'ler ile kelimeyi al\n",
    "# + kelimeyi tokenize et\n",
    "# + token sayısı == num_tokens ise list objesini (içinde tokenlar var) return et\n",
    "# + döngüden çıkıldı ise deneme sayısı aşılmış demektir uygun kelime bulunamamış demektir -> return null \n",
    "\n",
    "\n",
    "# UYARI: DATASET objesi ya da dataloader'da random index/sampling yapmalısın, A, B pairları hep ardışık şekilde!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "build_and_train_bert_model",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
