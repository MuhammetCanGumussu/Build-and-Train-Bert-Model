{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deneme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tr_wiki_ds():\n",
    "    # dosya path'lerini al\n",
    "    # regular expressionu hazırla\n",
    "    # dataset objesi oluştur (tablo, kolonlar: examples, titles, #characters)\n",
    "    # sırayla dosyaları(splitleri) aç\n",
    "        # dataset objesini/tabloyu doldurmaya başla\n",
    "    # işlem bitince tr-wiki67.arrow ismi ile kaydet (datasetdict objesinde başka splitler olmasın tek bir tane olsun)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import DatasetDict, Dataset\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[11, 8]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lens = list(map(len,[\"ali ata bak\", \"bak... !\"]))\n",
    "lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data/tr_wiki67/trwiki-67.test.raw',\n",
       " 'data/tr_wiki67/trwiki-67.train.raw',\n",
       " 'data/tr_wiki67/trwiki-67.val.raw']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prefix = \"data/tr_wiki67/trwiki-67.\"\n",
    "file_paths = [ f\"{prefix}{post_fix}.raw\" for post_fix in [\"test\", \"train\", \"val\"] ]\n",
    "file_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = {\"examples\":[], \"titles\":[], \"length\":[]}\n",
    "\n",
    "# go for every file\n",
    "for file in file_paths:\n",
    "    \n",
    "    # open file\n",
    "    with open(file) as f:\n",
    "        \n",
    "        # read its content\n",
    "        content = f.read()\n",
    "        \n",
    "        # with regular exp. find titles, examples(doc), and also measure length of examples\n",
    "        examples = list(map(str.strip, re.split(\"== .* ==\", content)[1:]))\n",
    "        titles = list(re.findall(\"== .* ==\", content))\n",
    "        length = list(map(len, examples))\n",
    "\n",
    "        # package these with dict object\n",
    "        temp_data_dict = {\"examples\":examples, \"titles\":titles, \"length\":length}\n",
    "    \n",
    "    # concat dict objects from diffrent files into one \n",
    "    data_dict[\"examples\"].extend(temp_data_dict[\"examples\"])\n",
    "    data_dict[\"titles\"].extend(temp_data_dict[\"titles\"])\n",
    "    data_dict[\"length\"].extend(temp_data_dict[\"length\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Gülek Kalesi ==\n",
      "Gülek Kalesi Mersin ilinde bir kale örenidir.\n",
      "Yerleşim.\n",
      "Kale Mersin ili Tarsus ilçesi kuzeyinde Toros Dağları üzerinde, Gülek Boğazına hakim bir noktada ve koordinatlarındadır. Kaleye Gülek beldesinden ayrılan 5 kilometrelik bir yolla ulaşılmaktadır. Deniz seviyesinden yüksekliği 1530 metre, denetlediği Gülek Boğazından yüksekliği ise 500 metredir. Tarsus’a olan uzaklığı 65 kilometre, Mersin’e olan uzaklığı ise 95 kilometredir.\n",
      "Tarihi.\n",
      "Kalenin yapılış tarihi bilinmese de, Kilikya Ermeni Krallığı döneminde yapıldığı veya esaslı bir onarımdan geçtiği anlaşılmaktadır. Bu devlete ait 1198-1199 dönemine tarihlenen bir taç giyme listesinde Gülek lordu Symbat’ın adı geçmektedir. Kalenin o dönemdeki adı ‘’Asakaliba’’ idi. Kale İç Anadolu ile Akdeniz Bölgesi arasındaki en önemli geçit olan Gülek Boğazını denetlemek amacıyla yapılmıştır. Ortaçağ'da bu boğazı geçenler kale komutanına ücret ödemek zorunda kalıyorlardı.\n",
      "Bölge daha sonra Memluk Devleti ve Osmanlı İmparatorluğu egemenliğinde kalmış, 19.yüzyılda Kavalalı Mehmet Ali Paşa isyanı sonrasında 1830 yıllarda Mehmet Ali Paşa’nın oğlu Kavalalı İbrahim Paşa tarafından işgal edilmiştir. Kalenin o dönemde de kullanıldığı anlaşılmaktadır. Tarihi kayıtlarda bu dönemde kalenin kuzey batısında bir de kurşun madeni olduğu belirtilmektedir.\n",
      "Kale.\n",
      "Kalenin giriş güneydeki kapıdandır. Surlar batı ve güney kesimdedir. Kuzey ve doğu kesimde çok dik yamaçlar olduğu için bu yönlerde tahkimata gerek duyulmamıştır. Surlarda kullanılan bosajlı taşlar Ermeni dönemi kalelerinin karakteristik malzemeleridir. Ancak daha yeni yapı izlerine de rastlanmaktadır. Bu yapılar daha kuzeyde yer alan İbrahim Paşa Tabyalarındaki yapıları andırmakta olup, büyük ihtimalle 19. yüzyılda İbrahim Paşa tarafından yaptırılmıştır. Kalenin doğu tarafında bir de sarnıç yer almaktadır\n",
      "1812\n"
     ]
    }
   ],
   "source": [
    "# lets print\n",
    "print(data_dict[\"titles\"][9625])\n",
    "print(data_dict[\"examples\"][9625])\n",
    "print(data_dict[\"length\"][9625])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gülek Kalesi Mersin ilinde bir kale örenidir.\n",
      "Kale Mersin ili Tarsus ilçesi kuzeyinde Toros Dağları üzerinde, Gülek Boğazına hakim bir noktada ve koordinatlarındadır. Kaleye Gülek beldesinden ayrılan 5 kilometrelik bir yolla ulaşılmaktadır. Deniz seviyesinden yüksekliği 1530 metre, denetlediği Gülek Boğazından yüksekliği ise 500 metredir. Tarsus’a olan uzaklığı 65 kilometre, Mersin’e olan uzaklığı ise 95 kilometredir.\n",
      "Kalenin yapılış tarihi bilinmese de, Kilikya Ermeni Krallığı döneminde yapıldığı veya esaslı bir onarımdan geçtiği anlaşılmaktadır. Bu devlete ait 1198-1199 dönemine tarihlenen bir taç giyme listesinde Gülek lordu Symbat’ın adı geçmektedir. Kalenin o dönemdeki adı ‘’Asakaliba’’ idi. Kale İç Anadolu ile Akdeniz Bölgesi arasındaki en önemli geçit olan Gülek Boğazını denetlemek amacıyla yapılmıştır. Ortaçağ'da bu boğazı geçenler kale komutanına ücret ödemek zorunda kalıyorlardı.\n",
      "Bölge daha sonra Memluk Devleti ve Osmanlı İmparatorluğu egemenliğinde kalmış, 19.yüzyılda Kavalalı Mehmet Ali Paşa isyanı sonrasında 1830 yıllarda Mehmet Ali Paşa’nın oğlu Kavalalı İbrahim Paşa tarafından işgal edilmiştir. Kalenin o dönemde de kullanıldığı anlaşılmaktadır. Tarihi kayıtlarda bu dönemde kalenin kuzey batısında bir de kurşun madeni olduğu belirtilmektedir.\n",
      "Kalenin giriş güneydeki kapıdandır. Surlar batı ve güney kesimdedir. Kuzey ve doğu kesimde çok dik yamaçlar olduğu için bu yönlerde tahkimata gerek duyulmamıştır. Surlarda kullanılan bosajlı taşlar Ermeni dönemi kalelerinin karakteristik malzemeleridir. Ancak daha yeni yapı izlerine de rastlanmaktadır. Bu yapılar daha kuzeyde yer alan İbrahim Paşa Tabyalarındaki yapıları andırmakta olup, büyük ihtimalle 19. yüzyılda İbrahim Paşa tarafından yaptırılmıştır. Kalenin doğu tarafında bir de sarnıç yer almaktadır\n"
     ]
    }
   ],
   "source": [
    "# after delete subtitles\n",
    "print(deneme[9625])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1356"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_dict[\"examples\"][99999])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(394134, 394134)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_dict[\"examples\"]), len(data_dict[\"titles\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>394134.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1280.671939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3259.907825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>204.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>453.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1136.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>193505.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              length\n",
       "count  394134.000000\n",
       "mean     1280.671939\n",
       "std      3259.907825\n",
       "min         0.000000\n",
       "25%       204.000000\n",
       "50%       453.000000\n",
       "75%      1136.000000\n",
       "max    193505.000000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.DataFrame(data_dict)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "504756354"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"length\"].sum() # ~~ 500_000_000 characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(df[\"length\"] == 0).sum() # number of zero values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = ~(df[\"length\"] == 0) # ~ symbol makes True -> False, False -> True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[mask]\n",
    "\n",
    "# now there will be no example that has not text-characters\n",
    "(df[\"length\"] == 0).sum() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Türkiye', 'İran55', 'ilişkileri', '55_55', '987']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_sentence = \"Türkiye-İran55 ilişkileri.55_55 - 987@\"\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "words = tokenizer.tokenize(example_sentence)\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_subtitles(example):\n",
    "    temp = []\n",
    "    # split lines but also keep new line character\n",
    "    for line in example.splitlines(True):\n",
    "        if len(tokenizer.tokenize(line)) > 5:\n",
    "            temp.append(line)\n",
    "    \n",
    "    if len(temp) != 0:\n",
    "        return \"\".join(temp)\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in our dataset (wiki), some lines are actually subtitles, this can make our continuation of \n",
    "# sentences worse (we will get pair of sentences for NSP task)\n",
    "# for this reason we need to go through all lines of examples and eliminate lines that are smaller than 5 words\n",
    "deneme = df[\"examples\"].map(delete_subtitles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "462     None\n",
       "603     None\n",
       "1013    None\n",
       "2936    None\n",
       "2993    None\n",
       "3527    None\n",
       "3871    None\n",
       "5961    None\n",
       "6990    None\n",
       "7270    None\n",
       "Name: examples, dtype: object"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deneme[deneme.isnull()].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sıralamalar\n",
      "İllere göre nüfus sayıları.\n",
      "Atabey kavramı aşağıdaki anlamlarda kullanılabilir\n",
      "Örümcek, bir eklembacaklı hayvan türüdür.\n",
      "Ayrıca şu anlamlara da gelebilir\n"
     ]
    }
   ],
   "source": [
    "print(data_dict[\"examples\"][3871])\n",
    "print(data_dict[\"examples\"][603])\n",
    "print(data_dict[\"examples\"][1013])\n",
    "print(data_dict[\"examples\"][7270])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "324"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deneme.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets convert this df object into dict again\n",
    "data_dict = df.to_dict(orient=\"list\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_subtitles(example):\n",
    "    \"\"\"\n",
    "    will be used as a map function\n",
    "    take every example-doc and delete if it has subtitles (if line count of word is < 5)\n",
    "    \"\"\"\n",
    "    temp = []\n",
    "    # split lines\n",
    "    for line in example.splitlines(True):\n",
    "        # if count of word is > 5 just keep this line, if not > 5, we will consider it as subtitle and not gonna keep it\n",
    "        if len(tokenizer.tokenize(line)) > 5:\n",
    "            temp.append(line)\n",
    "    \n",
    "    # in some cases there will be no lines that are bigger than 5\n",
    "    # also some examples may not have any text at all! (data construction failure)\n",
    "    if len(temp) != 0:\n",
    "        return \"\".join(temp)\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "\n",
    "def create_tr_wiki_ds():\n",
    "    \"\"\"\n",
    "    + there are 3 files (splits): trwiki-67.test.raw, trwiki-67.train.raw, trwiki-67.val.raw\n",
    "    + this function goes for every file\n",
    "    + finds titles and doc-examples\n",
    "    + concat everyting into one dict object\n",
    "    + there are problematic examples (no text(null) or some lines of examples are not actually paragraph\n",
    "      but subtitles)\n",
    "    + apply preprocess to handle these problems (delete subtitles and mask-filter-delete null examples)\n",
    "    + measure length of examples and return as a new column\n",
    "    + create dataset object and save it as .arrow format\n",
    "    \"\"\"\n",
    "    prefix = \"trwiki-67.\"\n",
    "    file_paths = [ f\"{prefix}{post_fix}.raw\" for post_fix in [\"test\", \"train\", \"val\"] ]\n",
    "\n",
    "    data_dict = {\"examples\":[], \"titles\":[]}\n",
    "\n",
    "    # go for every file\n",
    "    for file in file_paths:\n",
    "\n",
    "        # open file\n",
    "        with open(file) as f:\n",
    "\n",
    "            # read its content\n",
    "            content = f.read()\n",
    "\n",
    "            # with regular exp. find titles, examples(doc)\n",
    "            examples = list(map(str.strip, re.split(\"== .* ==\", content)[1:]))\n",
    "            titles = list(re.findall(\"== .* ==\", content))\n",
    "\n",
    "            # package these with dict object\n",
    "            temp_data_dict = {\"examples\":examples, \"titles\":titles}\n",
    "\n",
    "        # concat dict objects from diffrent files into one overall dict object\n",
    "        data_dict[\"examples\"].extend(temp_data_dict[\"examples\"])\n",
    "        data_dict[\"titles\"].extend(temp_data_dict[\"titles\"])\n",
    "\n",
    "        # convert data_dict object into dataframe object\n",
    "        df = pd.DataFrame(data_dict)\n",
    "        # lets delete lines of examples that are subtitles\n",
    "        df[\"examples\"] = df[\"examples\"].map(delete_subtitles)\n",
    "        # lets also delete-mask df by null values (examples that does not contain text at all)\n",
    "        mask = df[\"examples\"].isnull()\n",
    "        df = df[mask]\n",
    "\n",
    "        # lets measure length of examples (by char)\n",
    "        df[\"length\"] = df[\"examples\"].map(len)\n",
    "\n",
    "        # create hf dataset and save to disk\n",
    "        df = df.reset_index(drop=True)\n",
    "        dataset = Dataset.from_pandas(df)\n",
    "        dataset.save_to_disk(\"trwiki_dataset\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['examples', 'titles', 'length', 'deneme kolonu'], dtype='object')"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['examples', 'titles', 'length', 'deneme kolonu'],\n",
       "    num_rows: 394118\n",
       "})"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = df\n",
    "df2.reset_index(drop=True, inplace=True)\n",
    "Dataset.from_pandas(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetdeneme = Dataset.from_pandas(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['examples', 'titles', 'length', 'deneme kolonu', '__index_level_0__'],\n",
       "    num_rows: 394118\n",
       "})"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasetdeneme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11f38afa3a0b4e0d98a093a4dc8d9a35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/2 shards):   0%|          | 0/394118 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "datasetdeneme.save_to_disk(\"deneme\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "aaaa = load_from_disk(\"deneme\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['examples', 'titles', 'length', 'deneme kolonu', '__index_level_0__'],\n",
       "    num_rows: 394118\n",
       "})"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aaaa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['examples', 'titles', 'length'],\n",
       "    num_rows: 394118\n",
       "})"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aaaa.remove_columns([\"__index_level_0__\", \"deneme kolonu\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Örnek metin (dene.txt):\n",
    "== NGC 1710 == \n",
    "\n",
    "NGC 1710, Yeni Genel Katalog'da yer alan bir galaksidir. Gökyüzünde Aslan takımyıldızı yönünde bulunur. E-S0 tipi bir merceksi, eliptik galaksidir. Amerikan astronom Francis Leavenworth tarafından 1885 yılında 66,04 cm (26 inç) çaplı mercekli tip bir teleskopla keşfedilmiştir.\n",
    "\n",
    "== Şenol Gürşan == \n",
    "\n",
    "Şenol Gürşan, (d. 17 Ekim 1964, Pınarhisar, Kırklareli) Türk avukat ve siyasetçi.\n",
    "İstanbul Üniversitesi Hukuk Fakültesi'ni bitirmiş ve serbest avukat olarak çalışmıştır. Kırklareli İlim Yayma Cemiyeti Kuruculuğu ve Başkanlığı görevlerinde bulunmuştur.\n",
    "2009 yılında Adalet ve Kalkınma Partisi Kırklareli il yönetim kurulu üyesi olmuş, TBMM 24. dönem AK Parti Kırklareli milletvekili, Türkiye-Polonya Dostluk Grubu Başkanı ve TBMM KİT Komisyonu Sözcüsü olmuştur. Gelecek Partisi Kurucular Kurulu üyesi olup aynı zamanda partinin genel sekreteridir.\n",
    "İyi düzeyde Almanca bilen Gürşan, evli ve 2 çocuk babasıdır.\n",
    "\n",
    "== Bovenau == \n",
    "\n",
    "Bovenau Almanya'nın kuzeyinde Schleswig-Holstein eyaletinde, Rendsburg-Eckernförde iline bağlı bölge. Rendsburg'un 10 km. doğusunda yer alır. 26,2 km² yüzölçümüne sahiptir. Nüfusu, 17 Temmuz 2006 itibarıyla yaklaşık 1098 olarak tespit edilmiştir. Belediye Başkanlığı Jürgen Liebsch tarafından yürütülmektedir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "with open(\"trwiki-67.test.txt\") as fi:\n",
    "    content = fi.read()\n",
    "    articles = [ { \"title\": ti.strip(\"==\").strip(), \n",
    "                   \"text\": tx.strip()} \n",
    "                   for ti, tx in \n",
    "                   zip(re.findall(\"== .* ==\", content), re.split(\"== .* == \\n\\n\", content)[1:]) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " \"Şenol Gürşan, (d. 17 Ekim 1964, Pınarhisar, Kırklareli) Türk avukat ve siyasetçi.\\nİstanbul Üniversitesi Hukuk Fakültesi'ni bitirmiş ve serbest avukat olarak çalışmıştır. Kırklareli İlim Yayma Cemiyeti Kuruculuğu ve Başkanlığı görevlerinde bulunmuştur.\\n2009 yılında Adalet ve Kalkınma Partisi Kırklareli il yönetim kurulu üyesi olmuş, TBMM 24. dönem AK Parti Kırklareli milletvekili, Türkiye-Polonya Dostluk Grubu Başkanı ve TBMM KİT Komisyonu Sözcüsü olmuştur. Gelecek Partisi Kurucular Kurulu üyesi olup aynı zamanda partinin genel sekreteridir.\\nİyi düzeyde Almanca bilen Gürşan, evli ve 2 çocuk babasıdır.\\n\\n\",\n",
       " \"Bovenau Almanya'nın kuzeyinde Schleswig-Holstein eyaletinde, Rendsburg-Eckernförde iline bağlı bölge. Rendsburg'un 10\\xa0km. doğusunda yer alır. 26,2\\xa0km² yüzölçümüne sahiptir. Nüfusu, 17 Temmuz 2006 itibarıyla yaklaşık 1098 olarak tespit edilmiştir. Belediye Başkanlığı Jürgen Liebsch tarafından yürütülmektedir.\"]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"dene.txt\") as fi:\n",
    "    content = fi.read()\n",
    "    result = re.split(\"== .* == \\n\\n\", content)\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test For prepare create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51f1ea2c955a41f2a0e98339e0f4b724",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/9989 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4db53b86e02844d5aebec5628b8af3ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/2 shards):   0%|          | 0/383799 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6120819e874a48dc99d747cf9e0cd1d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/2 shards):   0%|          | 0/393794 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import data.tr_wiki67.prepare as prepare\n",
    "\n",
    "prepare.create_and_save_tr_wiki_ds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "build_and_train_bert_model",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
